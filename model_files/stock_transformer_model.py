# -*- coding: utf-8 -*-
"""smp500.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RMEiQo3dAiT8B3PhfZKGCqH5iApK18PS
"""

# mount Google Drive to get the kaggle.json
from google.colab import drive
drive.mount('/content/gdrive', readonly=True)

# install kaggle cli tool and move the kaggle.json to the correct directory
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp /content/gdrive/MyDrive/ml-project/kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json

# download the SMP 500 dataset which has stock data about the SMP 500 companies
# updated daily
!kaggle datasets download -d andrewmvd/sp-500-stocks

from zipfile import PyZipFile

# extract the data from the ZIP file
zipfile = PyZipFile('sp-500-stocks.zip')
zipfile.extractall()

import pandas as pd

# read stock data
df_stocks = pd.read_csv('sp500_stocks.csv', parse_dates=['Date'])
# drop extra column
df_stocks.drop(labels=['Adj Close'], axis=1, inplace=True)
# drop any rows that contain missing values
# df_stocks.dropna(inplace=True)
# Make symbol an index for identifying data associated with a stock
df_stocks.set_index('Symbol', inplace=True)
# group by the index 'Symbol'
grouped = df_stocks.groupby(level=0)
# create a dictionary of dataframes where the key is the stock symbol and the
# value is the stock data associated with the symbol
df_stocks_dict = {group: group_df for group, group_df in grouped}
# reset the symbol index, as it does not uniquely define each dataframe row
for df in df_stocks_dict.values():
  df.reset_index(drop=True, inplace=True)

mydf = df_stocks_dict['AAPL'].copy()
mydf.head()

mydf.info()

import matplotlib.pyplot as plt

# graph plots of data
fig, axs = plt.subplots(2, 3)
axs[0, 0].plot(mydf['Close'])
axs[0, 0].set_title('Close')
axs[0, 1].plot(mydf['High'])
axs[0, 1].set_title('High')
axs[0, 2].plot(mydf['Low'])
axs[0, 2].set_title('Low')
axs[1, 0].plot(mydf['Open'])
axs[1, 0].set_title('Open')
axs[1, 1].plot(mydf['Volume'])
axs[1, 1].set_title('Volume')

"""# Data Preprocessing"""

import numpy as np
import sklearn
from sklearn.preprocessing import MinMaxScaler

# make the data stationary, as this improves prediction accuracy for RNNs
def price_transform(df):
  df_log = df.apply(np.log)
  df_tf = df_log.apply(np.sqrt)
  df_shift = df_tf - df_tf.shift()
  return (df_shift, df_tf)

# make new dataframe with stationary data
# also get the unshifted but transformed (log & sqrt) data
mydf_stationary = mydf.copy()
mydf_stationary['Close'], mydf_close_tf = price_transform(mydf_stationary['Close'])
mydf_stationary['High'], mydf_high_tf = price_transform(mydf_stationary['High'])
mydf_stationary['Low'], mydf_low_tf = price_transform(mydf_stationary['Low'])
mydf_stationary['Open'], mydf_open_tf = price_transform(mydf_stationary['Volume'])

# Volume does not change like the prices, so normalize it instead
sc = MinMaxScaler(feature_range = (0, 1))
scaler = sc.fit(mydf_stationary['Volume'].to_numpy().reshape(-1, 1))
mydf_stationary['Volume'] = scaler.transform(mydf_stationary['Volume'].to_numpy().reshape(-1, 1))

# Drop null values and Date
mydf_stationary.dropna(inplace=True)
mydf_stationary.drop('Date', axis=1, inplace=True)

# graph the new stationary data
fig, axs = plt.subplots(2, 3)
axs[0, 0].plot(mydf_stationary['Close'])
axs[0, 0].set_title('Close')
axs[0, 1].plot(mydf_stationary['High'])
axs[0, 1].set_title('High')
axs[0, 2].plot(mydf_stationary['Low'])
axs[0, 2].set_title('Low')
axs[1, 0].plot(mydf_stationary['Open'])
axs[1, 0].set_title('Open')
axs[1, 1].plot(mydf_stationary['Volume'])
axs[1, 1].set_title('Volume')
mydf_stationary.info()

print(mydf_stationary['Close'].head())
print(mydf_close_tf.head())

"""# One Day in the Future"""

def preprocess_lstm(sequence, n_steps, n_features):
    X, y = list(), list()
    for i in range(len(sequence)):
        # find the end of this pattern
        end_ix = i + n_steps
        # check if it is beyond the sequence
        if end_ix >= len(sequence):
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
        X.append(seq_x)
        y.append(seq_y)

    X = np.array(X)
    y = np.array(y)

    X = X.reshape((X.shape[0], X.shape[1], n_features))
    return X, y

# choose the number of days on which to base our predictions
n_base_instances = 60

# number of features in the dataframe
n_features = 1

# get the inputs and targets in the shape (batch_size, timesteps, features) and (batch_size), respectively
X, y = preprocess_lstm(mydf_stationary['Close'].to_numpy(), n_base_instances, n_features)

print(X.shape)
print(y.shape)

test_days = 365

# split into train and test sets
X_train, y_train = X[:-test_days], y[:-test_days]
X_test, y_test = X[-test_days:], y[-test_days:]

# get the original values for Close price
train_original = mydf['Close'].iloc[:-test_days]
test_original = mydf['Close'].iloc[-test_days:]

# graph original Close price values
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel('Dates')
plt.ylabel('Close Prices')
plt.plot(train_original, 'b', label='Train data')
plt.plot(test_original, 'g', label='Test data')
plt.legend()

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# LSTM with 1 LSTM layer and 1 Dense layer
# predicts 1 day in the future
def vanilla_LSTM():
    lstm = keras.Sequential()
    lstm.add(layers.LSTM(units=50, input_shape=(n_base_instances, n_features)))
    lstm.add(layers.Dense(1))
    return lstm

lstm = vanilla_LSTM()
lstm.summary()
lstm.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=[tf.keras.metrics.MeanAbsoluteError()])

lstm.fit(X_train, y_train, epochs=15, batch_size = 32)

# Evaluate the model on the test data
print("Evaluate on test data")
results = lstm.evaluate(X_test, y_test, batch_size=32)
print("Test MSE:", results[0])
print("Test MAE:", results[1])

# predict using X_test
y_pred = lstm.predict(X_test)

# create a dataframe from the model predictions
pred_data = pd.DataFrame(y_pred[:, 0], test_original.index, columns=['Close'])

# Reverse the stationary operations

# Reverse the shift operation
pred_data['Close'] = pred_data['Close'] + mydf_close_tf.shift().values[-test_days:]

# Reverse the sqrt and log operations
pred_data = pred_data.apply(np.square)
pred_data = pred_data.apply(np.exp)


# Plot actual prices vs predicted prices
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel('Dates')
plt.ylabel('Close')
plt.plot(test_original,'b',label='Actual prices')
plt.plot(pred_data, 'orange',label='Predicted prices')
plt.title('AAPL Stock Price')
plt.legend()

train_original = mydf['Close'].iloc[:-test_days]
test_original = mydf['Close'].iloc[-test_days:]

# plot train data, test data, and predicted values
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel('Dates')
plt.ylabel('Close Prices')
plt.plot(train_original, 'b', label='Train data')
plt.plot(test_original, 'g', label='Test data')
plt.plot(pred_data, 'orange', label='Prediction')
plt.title('AAPL Stock Price')
plt.legend()

"""## The below transformer is flawed, as the predictions are only zeros.
Adapted from reference #2.
"""

import tensorflow as tf
from tensorflow.keras import layers

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0, attention_axes=None):
    # Attention and Normalization
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout, attention_axes=attention_axes
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res

def build_model(
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    """
    Creates final model by building many transformer blocks.
    """
    n_timesteps, n_features, n_outputs = 60, 1, 1
    inputs = keras.Input(shape=(n_timesteps, n_features))
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format="channels_first")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)

    outputs = layers.Dense(n_outputs, activation='relu')(x)
    return tf.keras.Model(inputs, outputs)

input_shape = X_train.shape[1:]
print(input_shape)

transformer = build_model(
    head_size=128,
    num_heads=4,
    ff_dim=2,
    num_transformer_blocks=4,
    mlp_units=[256],
    dropout=0.10,
    mlp_dropout=0.10,
)

transformer.compile(
    loss="mse",
    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
    metrics=["mae", 'mape'],
)
transformer.summary()

callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]

transformer.fit(
    X_train,
    y_train,
    validation_split=0.2,
    epochs=150,
    batch_size=64,
    callbacks=callbacks,
)

results = transformer.evaluate(X_test, y_test, verbose=1)

print("Test MSE:", results[0])
print("Test MAE:", results[1])

# Prediction
y_pred = transformer.predict(X_test)
print(y_pred.shape)

# Graph test data vs predicted values
plt.figure(figsize=(10,6))
plt.grid(True)
plt.plot(y_test, label='Orginal data - transformed')
plt.plot(y_pred, color='red', label='Predictions - transformed')
plt.xlabel('Time (days)')
plt.ylabel('Close Prices amplitude in the transformed space')
plt.title('Original data vs predictions in the transformed space')

# predict using X_test
y_pred = transformer.predict(X_test)

# create a dataframe from the model predictions
pred_data = pd.DataFrame(y_pred, test_original.index, columns=['Close'])

# Reverse the stationary operations

# Reverse the shift operation
pred_data['Close'] = pred_data['Close'] + mydf_close_tf.shift().values[-test_days:]

# Reverse the sqrt and log operations
pred_data = pred_data.apply(np.square)
pred_data = pred_data.apply(np.exp)


# Plot actual prices vs predicted prices
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel('Dates')
plt.ylabel('Close Prices')
plt.plot(test_original,'b',label='Actual prices')
plt.plot(pred_data, 'orange',label='Predicted prices')
plt.title('AAPL Stock Price')

# plot train data, test data, and predicted values
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel('Dates')
plt.ylabel('Close Prices')
plt.plot(train_original, 'b', label='Train data')
plt.plot(test_original, 'g', label='Test data')
plt.plot(pred_data, 'orange', label='Prediction')
plt.title('AAPL Stock Price')
plt.legend()

"""# Multiple Days in the Future"""

def preprocess_multistep_model(sequence, n_steps_in, n_steps_out, n_features):
    X, y = list(), list()
    for i in range(len(sequence)):
        # find the end of this pattern
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        # check if it is beyond the sequence
        if out_end_ix > len(sequence):
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequence.iloc[i:end_ix], sequence.iloc[end_ix:out_end_ix]
        X.append(seq_x.to_numpy())
        y.append(seq_y.to_numpy())

    X = np.array(X)
    y = np.array(y)

    X = X.reshape((X.shape[0], X.shape[1], n_features))

    return X, y

# number of days in the future to predict
n_steps_out = 30

# number of days on which to base the prediction
n_base_instances = 30

# number of features in the dataframe
n_features = 1

# X, y = preprocess_multistep_model(mydf_stationary, n_base_instances, n_steps_out)
X, y = preprocess_multistep_model(mydf_stationary['Close'], n_base_instances, n_steps_out, n_features)

print(X.shape)
print(y.shape)

test_days = 365

# split into train and test sets
X_train, y_train = X[:-test_days], y[:-test_days]
X_test, y_test = X[-test_days:], y[-test_days:]

train_original = mydf['Close'].iloc[:-test_days]
test_original = mydf['Close'].iloc[-test_days:]

# graph train data and test data together
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel('Dates')
plt.ylabel('Close Prices')
plt.plot(train_original, 'b', label='Train data')
plt.plot(test_original, 'g', label='Test data')
plt.legend()

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# LSTM model to predict multiple days in the future
# consists of 1 LSTM layer and 1 Dense layer
def vanilla_multistep_LSTM():
    lstm = keras.Sequential()
    lstm.add(layers.LSTM(units=50, input_shape=(n_base_instances, 1)))
    lstm.add(layers.Dense(n_steps_out))
    return lstm

lstm = vanilla_multistep_LSTM()
lstm.summary()
lstm.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=[tf.keras.metrics.MeanAbsoluteError()])

lstm.fit(X_train, y_train, epochs=15, batch_size=32)

results = lstm.evaluate(X_test, y_test, batch_size=32)

print("Test MSE:", results[0])
print("Test MAE:", results[1])

# Prediction
y_pred = lstm.predict(X_test)

# the_day is the day from which we will study the n_steps_out-th dayS of prediction into
# the future. Note: The first day start at index 0
the_day = 0
y_pred_days = y_pred[the_day,:]

# graph the true test data and the predicted values
plt.figure(figsize=(10,6))
plt.grid(True)
plt.plot(y_test[the_day,:],label='Orginal data - transformed')
plt.plot(y_pred_days, color='red',label='Predictions - transformed')
plt.xlabel('Time (days)')
plt.ylabel('Close Prices amplitude in the transformed space')
plt.title('Original data vs predictions in the transformed space')

# Reverse the staionary operations

# get the cumulative sum of the predicted days
pred_diff_cumsum = y_pred_days.cumsum()

# get the base number
base_number = mydf_close_tf.values[-test_days+the_day+n_base_instances-1]
# get the index range of the true values corresponding to the predicted days
idx = test_original.iloc[the_day:the_day+n_steps_out].index

# get the unshifted values of the prediction
pred_tf = pd.Series(base_number, index=idx)
pred_tf = pred_tf.add(pred_diff_cumsum, fill_value=0)
print(pred_tf)

# Reverse the sqrt and log operations
pred_log = pred_tf.apply(np.square)
pred = pred_log.apply(np.exp)
print(pred)

# Plot actual prices vs predicted prices
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel('Dates')
plt.ylabel('Close Prices')
plt.plot(test_original.iloc[max(0,the_day-30):the_day+n_steps_out],'b',label='Actual prices')
plt.plot(pred, '-o',color='orange',label='Predicted prices')
plt.title('AAPL Stock Price')

plt.legend()

"""### I cannot figure out how to make the below model output predictions in the right shape (30, 5).
Considering the problem, it may not make sense to use a transformer model at all, as outputs of the decoder should be fed back through as inputs. If the inputs and outputs are different shapes, then this would cause errors. In addition, this model was based on the transformer for classification in the Keras documentation (shown in references), so there might be some error in adapting it to a regression problem.
"""

import tensorflow as tf
from tensorflow.keras import layers

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Attention and Normalization
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(res)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    return x + res

def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
    n_outputs=5
):
    """
    Creates final model by building many transformer blocks.
    """
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format="channels_last")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)

    outputs = layers.Dense(n_outputs, activation='relu')(x)
    return tf.keras.Model(inputs, outputs)

input_shape = X_train.shape[1:]
print(input_shape)

transformer = build_model(
    input_shape,
    head_size=256,
    num_heads=4,
    ff_dim=4,
    num_transformer_blocks=4,
    mlp_units=[128],
    dropout=0.10,
    mlp_dropout=0.10,
    n_outputs=30
)

transformer.compile(
    loss="mse",
    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
    metrics=["mae"],
)
transformer.summary()

callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]

transformer.fit(
    X_train,
    y_train,
    validation_split=0.2,
    epochs=150,
    batch_size=64,
    callbacks=callbacks,
)

results = transformer.evaluate(X_test, y_test, verbose=1)

print("Test MSE:", results[0])
print("Test MAE:", results[1])

# Prediction
y_pred = transformer.predict(X_test)

# the_day is the day from which we will study the n_steps_out-th dayS of prediction into
# the future. Note: The first day start at index 0
the_day = 0
y_pred_days = y_pred[the_day,:]
print(y_pred_days)

# graph test data and predicted values
plt.figure(figsize=(10,6))
plt.grid(True)
plt.plot(y_test[the_day,:],label='Orginal data - transformed')
plt.plot(y_pred_days, color='red',label='Predictions - transformed')
plt.xlabel('Time (days)')
plt.ylabel('Close Prices amplitude in the transformed space')
plt.title('Original data vs predictions in the transformed space')

# Reverse the staionary operations

# get the cumulative sum of the predicted days
pred_diff_cumsum = y_pred_days.cumsum()

# get the base number
base_number = mydf_close_tf.values[-test_days+the_day+n_base_instances-1]
# get the index range of the true values corresponding to the predicted days
idx = test_original.iloc[the_day:the_day+n_steps_out].index

# get the unshifted values of the prediction
pred_tf = pd.Series(base_number, index=idx)
pred_tf = pred_tf.add(pred_diff_cumsum,fill_value=0)

print(pred_tf)

# Reverse the sqrt and log operations
pred_log = pred_tf.apply(np.square)
pred = pred_log.apply(np.exp)
print(pred)

# Plot actual prices vs predicted prices
plt.figure(figsize=(10,6))
plt.grid(True)
plt.xlabel('Dates')
plt.ylabel('Close Prices')
plt.plot(test_original.iloc[max(0,the_day-30):the_day+n_steps_out],'b',label='Actual prices')
plt.plot(pred, '-o',color='orange',label='Predicted prices')
plt.title('AAPL Stock Price')

plt.legend()

"""# References

Code adapted from:
1. https://www.kaggle.com/code/thibauthurson/stock-price-prediction-with-lstm-multi-step-lstm
2. https://medium.com/@mskmay66/transformers-vs-lstm-for-stock-price-time-series-prediction-3a26fcc1a782
3. https://keras.io/examples/timeseries/timeseries_classification_transformer/
"""